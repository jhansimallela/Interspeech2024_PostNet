# -*- coding: utf-8 -*-
"""Unsupervised_TDNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xrqODXYErF-PmOTYmwuErhae4dsYahiA
"""

# from google.colab import drive
# drive.mount('/content/drive')

# Import section
import os
import keras
import scipy.io
import statistics
import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras import backend as K
from collections import Counter
from keras.optimizers import Adam
from keras.models import Sequential
from sklearn.metrics import f1_score
from keras.utils import to_categorical
from keras.losses import binary_crossentropy
from sklearn.model_selection import train_test_split
from keras.callbacks import ModelCheckpoint ,EarlyStopping
from keras.layers import TimeDistributed, Dense, BatchNormalization, Dropout, Masking

#loading data

def dl(path):
  data = scipy.io.loadmat(path)
  print(data.keys())
  x = data['features']; y = data['labels'];
  if 'test' in path and 'TDNN' in path:
    w = data['w']
    test_ind = data['test_ind']
    return x, y, w, test_ind
  elif 'test' in path and 'TDNN' not in path:
    w = data['w']
    return x, y, w
  else:
    return x, y.T

def convert_function(arr):
  temp=[]
  for i in arr:
    if i>0.5:
      temp.append(1)
    else:
      temp.append(0)
  return temp

def calculate_accuracy(arr1, arr2):
  count=0
  for itr1, itr2 in zip(arr1, arr2):
    if itr1==itr2:
      count+=1
  return count/len(arr1)

def make_partitions(arr_words, arr_labels):
  v=[]
  np.array(v)
  temp=[]
  for i in range(len(arr_words)-1):
    word=arr_words[i]
    next_word=arr_words[i+1]
    temp.append(arr_labels[i][0])
    if word!=next_word:
      numpy_temp=np.array(temp)
      temp_max=np.amax(numpy_temp)
      numpy_temp=np.divide(numpy_temp, temp_max)
      v=np.concatenate((v, numpy_temp), axis=None)
      temp.clear()
    if (i==len(arr_words)-2):
      temp.append(arr_labels[i+1][0])
      numpy_temp=np.array(temp)
      temp_max=np.amax(numpy_temp)
      numpy_temp=np.divide(numpy_temp, temp_max)
      v=np.concatenate((v, numpy_temp), axis=None)
      temp.clear()
  v1=[]
  for i in v:
    if i==1:
      v1.append(1)
    else:
      v1.append(0)
  return v1


def make_partitions2(arr_words, arr_labels):
  v=[]
  np.array(v)
  temp=[]
  for i in range(len(arr_words)-1):
    word=arr_words[i]
    next_word=arr_words[i+1]
    temp.append(arr_labels[i])
    if word!=next_word:
      numpy_temp=np.array(temp)
      temp_max=np.amax(numpy_temp)
      numpy_temp=np.divide(numpy_temp, temp_max)
      v=np.concatenate((v, numpy_temp), axis=None)
      temp.clear()
    if (i==len(arr_words)-2):
      temp.append(arr_labels[i+1])
      numpy_temp=np.array(temp)
      temp_max=np.amax(numpy_temp)
      numpy_temp=np.divide(numpy_temp, temp_max)
      v=np.concatenate((v, numpy_temp), axis=None)
      temp.clear()
  v1=[]
  for i in v:
    if i==1:
      v1.append(1)
    else:
      v1.append(0)
  return v1

def sampling(args):
    z_mean, z_log_sigma = args
    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=0.004)
    return z_mean + K.exp(z_log_sigma) * epsilon

def normalization(feats,avg,std):
  ii=0
  for v in feats:
    # print(len(v))
    feats[ii] = np.divide((v-avg),std)
    ii = ii+1
  return feats

WPP_accuracies_avg = []; WoPP_accuracies_avg = [];WPP_all =[]; WoPP_all=[];names=[]
WPP_accuracies_std = []; WoPP_accuracies_std = [];

xtrain1, ytrain1 = dl("GER_AC_features_TDNN.mat")
xtest_ac, ytest, wtest, test_ind = dl("GER_test_AC_features_TDNN.mat")
xt, yt, wt = dl("GER_test_AC_features.mat")

print("TDNN train data features:", xtrain1.shape, "\nTDNN train data labels:", ytrain1.shape)
print("TDNN test data features:", xtest_ac.shape, "\nTDNN test data labels: ",ytest.shape, "\nTDNN test data syllable count: ",wtest.shape, "\nNormal test data features: ",xt.shape, "\nNormal test data labels: ",yt.shape, "\nNormal test data word count: ", wt.shape)

woPP=[]; wPP=[]
accuracy_all_folds_wopp = []; accuracy_all_folds_wpp = []; f1score_all_folds_wopp = []; f1score_all_folds_wpp = [];
accuracy_all_folds_wopp_a = [];  accuracy_all_folds_wpp_a  = [];  f1score_all_folds_wopp_a  = [];  f1score_all_folds_wpp_a  = [];


highest_accuracy = 0.0

ytest_categorical = to_categorical(ytest, num_classes=2)

yt = yt.flatten()
wtest = wtest.flatten()
test_ind = test_ind.flatten()
# print(yt.shape, wtest.shape, test_ind.shape)

cc=[]
yy = ytrain1.flatten()
# print(yy)
c = 0
for i in range(0,len(yy)-1):
  if yy[i]!=-1.0:
    c+=1
  else:
    cc.append(c)
    c=0
    continue
# print(cc)

original_list = cc

expanded_list = []
for num in original_list:
    if num != 0:
        expanded_list.extend([num] * num)

# print("Expanded list:", expanded_list)

indices_list = []
counter = 1
for num in original_list:
    if num != 0:
        indices_list.extend([counter] * num)
        counter += 1

# print("Indices list:", indices_list)

wtrain1 = indices_list

original_dim = 38

woPP = []
wPP = []
accuracy_all_folds_wopp_a = []
accuracy_all_folds_wpp_a = []
f1score_all_folds_wopp_a = []
f1score_all_folds_wpp_a = []
accuracy_all_folds_wpp_a1 = []
z = 0
def custom_loss(y_true, y_pred1, wcou):
    y = 0


    y_pred = K.reshape(y_pred1, (-1, K.int_shape(y_pred1)[-1]))
    y_pred = y_pred[:, -1]
    #K.print_tensor(y_pred)
    total_loss = 0.0


    for x in wcou:
      group_sum = 0.0
      loss = 0.0

      y_p = y_pred[y:y + x]

      if y_p is None or K.int_shape(y_p)[0] == 0:
          y += 32
          continue

      max_ind = K.argmax(y_p)
      y_t = K.one_hot(max_ind, x)

      #print(K.print_tensor(y_t), K.print_tensor(y_p))

      # Calculate the loss for the group
      loss = binary_crossentropy(y_t, y_p)

      #print(K.print_tensor(loss))

      total_loss += loss
      y += 32


    return total_loss

j=0
#xtra_ac, xval_ac, ytra_ac, yval_ac = train_test_split(xtrain1, ytrain1, test_size=0.2, random_state=42)

train_size = xtrain1.shape[0]
wcou = Counter(wtrain1).values()
wcou = list(wcou)
print("wcou",len(wcou))

wcou_tr = wcou[:96]
wcou_val = wcou[96:]


max_sequence_length = 5


for j in range(0,1): # folds

  xval_ac = xtrain1[(train_size*j)//5:(train_size*(j+1))//5];
  #print(xval_ac.shape)
  yval_ac = ytrain1[(train_size*j)//5:(train_size*(j+1))//5]
  xtra_ac = np.concatenate((xtrain1[:(train_size*j)//5], xtrain1[((train_size*(j+1))//5):]), axis=0);
  ytra_ac = np.concatenate((ytrain1[:(train_size*j)//5], ytrain1[((train_size*(j+1))//5):]), axis=0)
  # wtr = w[(train_size*j)//5:(train_size*(j+1))//5]
  # wval = np.concatenate((w[:(train_size*j)//5], w[((train_size*(j+1))//5):]), axis=0);

  # wcou_tr = list(Counter(wtr).values())
  # wcou_val = list(Counter(wval).values())

  print(len(wcou_tr), len(wcou_val))

  model = Sequential()
  model.add(Masking(mask_value=-1., input_shape=(max_sequence_length, original_dim)))
  model.add(TimeDistributed(Dense(64, activation='relu')))
  model.add(TimeDistributed(BatchNormalization()))
  model.add(TimeDistributed(Dropout(0.45)))
  model.add(TimeDistributed(Dense(32, activation='relu')))
  model.add(TimeDistributed(BatchNormalization()))
  model.add(TimeDistributed(Dropout(0.3)))
  model.add(TimeDistributed(Dense(16, activation='relu')))
  model.add(TimeDistributed(Dense(4, activation='relu')))
  model.add(TimeDistributed(Dense(2, activation='softmax', name='stress')))

  y_true = ytra_ac
  z=0
  sequence_length = 5
  num_sequences = len(xtra_ac) // sequence_length

  xtra_ac_reshaped = xtra_ac[:num_sequences * sequence_length].reshape((num_sequences, sequence_length, 38))


  i = 0; j = 0; label = 0
  ytra_ac_modified = np.zeros((num_sequences, sequence_length, 2), dtype=int)

  for i in range(num_sequences):
      for j in range(sequence_length):
          label = int(ytra_ac[i * sequence_length + j])
          ytra_ac_modified[i, j, label] = 1

  num_sequences_val = len(xval_ac) // sequence_length

  xval_ac_reshaped = xval_ac[:num_sequences_val * sequence_length].reshape((num_sequences_val, sequence_length, 38))

  i=0; j = 0; label = 0

  yval_ac_modified = np.zeros((num_sequences_val, sequence_length, 2), dtype=int)

  for i in range(num_sequences_val):
      #print(i)
      for j in range(sequence_length):
          label = int(yval_ac[i * sequence_length + j])
          yval_ac_modified[i, j, label] = 1


  # Early stopping callback
  #early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

  # Train the model with early stopping
  #bs = len(wtra_ac)
  #history = model.fit(xtra_ac_reshaped, ytra_ac_modified, epochs=2, batch_size=4, validation_data=(xval_ac_reshaped, yval_ac_modified), callbacks=[early_stopping])


  #/ Compile the model
  batch_size = 32
  epochs = 1

  patience = 3
  best_val_loss = float('inf')
  counter = 0
  best_tr = 0
  best_bat = 0
  for epoch in range(epochs):
      print(f"\nEpoch {epoch + 1}/{epochs}")

      for i in range(0, len(xtra_ac_reshaped), batch_size):
          x_batch = xtra_ac_reshaped[i:i + batch_size]
          y_batch = ytra_ac_modified[i:i + batch_size]
          wcou_batch = wcou[i:i + batch_size]
          wcou_batch = np.array(wcou_batch)


          model.compile(optimizer=Adam(learning_rate=0.0005), loss=lambda y_true, y_pred: custom_loss(y_true, y_pred, wcou_batch), metrics=['accuracy'])


          loss = model.train_on_batch(x_batch, y_batch, sample_weight=wcou_batch)
          if (loss[-1]*100)>best_bat and (loss[-1]*100)>78:
            best_tr = loss[-1]*100
            model.save_weights('ger_best_batch_weights1.h5')

          print(f"Batch {i // batch_size + 1}/{len(xtra_ac_reshaped) // batch_size} - Loss: {loss}")

      # Validation
      val_loss, val_accuracy = model.evaluate(xval_ac_reshaped, yval_ac_modified, batch_size=batch_size)
      print(f"Validation Loss: {val_loss} - Validation Accuracy: {val_accuracy}")

      # Training set accuracy
      train_loss, train_accuracy = model.evaluate(xtra_ac_reshaped, ytra_ac_modified, batch_size=batch_size)
      print(f"Training Loss: {train_loss} - Training Accuracy: {train_accuracy}")


      if val_loss < best_val_loss:
          best_val_loss = val_loss
          counter = 0
      else:
          counter += 1

      if counter >= patience:
          print("Early stopping triggered. Restoring best weights.")
          break

  num_sequences_test = len(xtest_ac) // sequence_length

  xtest_ac_modified = xtest_ac[:num_sequences_test * sequence_length].reshape((num_sequences_test, sequence_length, 38))
  i=0; j = 0; label = 0

  ytest_ac_modified = np.zeros((num_sequences_test, sequence_length, 2), dtype=int)

  pred_output1= model.predict(xtest_ac_modified)
  # print(pred_output1)
  # Your input array
  array_of_arrays = pred_output1

  flattened_array = array_of_arrays.reshape(-1, array_of_arrays.shape[-1])

  result_array_ = [flattened_array[i] for i in range(len(flattened_array)) if i not in test_ind]
  #print(result_array)
  result_array = np.array([array.tolist() for array in result_array_])
  # print(result_array, yt)
  # print(result_array.shape, yt.shape)
  y_pred_a = np.where(result_array[:,1] > 0.5, 1, 0)

  accuracy = calculate_accuracy(y_pred_a, yt)
  print("============......",accuracy)
  post_labels=make_partitions(wtest, result_array)
  post_l = np.array(post_labels)
  swapped_array = np.where(post_l == 0, 1.0, 0.0)

  accuracy_xtest_a = calculate_accuracy(post_l, yt)
  accuracy_xtest_a1 = calculate_accuracy(swapped_array, yt)
  print(accuracy_xtest_a,"original")
  print(accuracy_xtest_a1,"swapped")
  # F1_score_WoPP_a = f1_score(ytest1, y_pred_a)
  # F1_score_WPP_a = f1_score(ytest1, post_labels)

  accuracy_all_folds_wopp_a.append(round(accuracy * 100, 3))
  accuracy_all_folds_wpp_a.append(round(accuracy_xtest_a * 100, 3))
  accuracy_all_folds_wpp_a1.append(round(accuracy_xtest_a1 * 100, 3))
  # f1score_all_folds_wopp_a.append(round(F1_score_WoPP_a * 100, 3))
  # f1score_all_folds_wpp_a.append(round(F1_score_WPP_a * 100, 3))