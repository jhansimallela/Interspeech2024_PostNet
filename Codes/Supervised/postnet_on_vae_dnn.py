# -*- coding: utf-8 -*-
"""PostNet on VAE_DNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YL19gOpDsTHLvPo00Rf7clJIlZktGnSF
"""

# from google.colab import drive
# drive.mount('/content/drive')

import os
import keras
import scipy.io
import statistics
import numpy as np
from keras import layers
import tensorflow as tf
import keras.backend as K
from tensorflow import keras
from collections import Counter
from tensorflow.keras import layers
from sklearn.metrics import f1_score
from keras.optimizers import legacy, Adam
from keras.models import Model, Sequential
from keras.preprocessing.sequence import pad_sequences
from keras.callbacks import ModelCheckpoint ,EarlyStopping
from keras.layers import TimeDistributed, Dense, BatchNormalization, Dropout, Masking, Lambda, Input, concatenate

#loading data

def dl(path):
  data = scipy.io.loadmat(path)
  print(data.keys())
  x = data['features']; y = data['labels'];
  if 'test' in path and 'TDNN' in path:
    w = data['w']
    test_ind = data['test_ind']
    return x, y, w, test_ind
  elif 'test' in path and 'TDNN' not in path:
    w = data['w']
    return x, y, w
  else:
    return x, y.T

def normalization(feats,avg,std):
  ii=0
  for v in feats:
    # print(len(v))
    feats[ii] = np.divide((v-avg),std)
    ii = ii+1
  return feats

# post processing the labels
def make_partitions(arr_words, arr_labels):
  v=[]
  np.array(v)
  temp=[]
  for i in range(len(arr_words)-1):
    word=arr_words[i]
    next_word=arr_words[i+1]
    temp.append(arr_labels[i])
    if word!=next_word:
      numpy_temp=np.array(temp)
      temp_max=np.amax(numpy_temp)
      numpy_temp=np.divide(numpy_temp, temp_max)
      v=np.concatenate((v, numpy_temp), axis=None)
      temp.clear()
    if (i==len(arr_words)-2):
      temp.append(arr_labels[i+1])
      numpy_temp=np.array(temp)
      temp_max=np.amax(numpy_temp)
      numpy_temp=np.divide(numpy_temp, temp_max)
      v=np.concatenate((v, numpy_temp), axis=None)
      temp.clear()
  v1=[]
  for i in v:
    if i==1:
      v1.append(1)
    else:
      v1.append(0)
  return v1

#Function to compute classification accuracy
def calculate_accuracy(arr1, arr2):
  count=0
  for itr1, itr2 in zip(arr1, arr2):
    if itr1==itr2:
      count+=1
  return count/len(arr1)

def sampling(args):
    z_mean, z_log_sigma = args
    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=0.004)
    return z_mean + K.exp(z_log_sigma) * epsilon

filee = 'GER_AC_features_TDNN.mat'

print('Classification with::::::',os.path.basename(filee))

train_path = filee;
xtrain1, ytrain1 = dl(train_path);

train_size = xtrain1.shape[0]
print(train_size)

avg_trainfeat1=np.mean(xtrain1, axis=0)
std_trainfeat1=np.std(xtrain1, axis=0)

test_path = 'GER_test_AC_features.mat'
print('test file:::::::',os.path.basename(test_path))

xtest1, ytest1, wtest1 = dl(test_path);

xtest_ac = normalization(xtest1,avg_trainfeat1,std_trainfeat1)

xtrain1 = normalization(xtrain1,avg_trainfeat1,std_trainfeat1)
print(xtrain1.shape)

woPP=[]; wPP=[]
accuracy_all_folds_wopp = []; accuracy_all_folds_wpp = []; f1score_all_folds_wopp = []; f1score_all_folds_wpp = [];
accuracy_all_folds_wopp_a = [];  accuracy_all_folds_wpp_a  = [];  f1score_all_folds_wopp_a  = [];  f1score_all_folds_wpp_a  = [];


print(xtrain1.shape)
print(ytrain1.shape)

highest_accuracy = 0.0

def convert_function(arr):
  temp=[]
  for i in arr:
    if i>0.5:
      temp.append(1)
    else:
      temp.append(0)
  return temp

# original_dim = 38
# intermediate_dim = 64
# latent_dim = 38
# highest_accuracy = 0.0
# woPP = []; wPP = []; f1score_woPP =[]; f1score_wPP=[]

# for j in range(0,1): # folds

#   xval_ac = xtrain1[(train_size*j)//5:(train_size*(j+1))//5];
#   print(xval_ac.shape)
#   yval_ac = ytrain1[(train_size*j)//5:(train_size*(j+1))//5]
#   xtra_ac = np.concatenate((xtrain1[:(train_size*j)//5], xtrain1[((train_size*(j+1))//5):]), axis=0);
#   ytra_ac = np.concatenate((ytrain1[:(train_size*j)//5], ytrain1[((train_size*(j+1))//5):]), axis=0)


#   inputs = keras.Input(shape=(original_dim,))
#   h = layers.Dense(intermediate_dim, activation='relu')(inputs)
#   z_mean = layers.Dense(latent_dim)(h)
#   z_log_sigma = layers.Dense(latent_dim)(h)


#   z = layers.Lambda(sampling)([z_mean, z_log_sigma])

#   print (z.shape)
#   encoder = keras.Model(inputs, [z_mean, z_log_sigma, z], name='encoder')


# # Create decoder
#   latent_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')
#   x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)
#   output = layers.Dense(original_dim, activation='sigmoid')(x)
#   decoder = keras.Model(latent_inputs, output, name='decoder')

# # instantiate VAE model
#   outputs = decoder(encoder(inputs)[2])
#   vae = keras.Model(inputs, outputs)


#   reconstruction_loss = keras.losses.mean_squared_error(inputs, outputs)
#   reconstruction_loss *= original_dim
#   kl_loss = 1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma)
#   kl_loss = K.sum(kl_loss, axis=-1)
#   kl_loss *= -0.5
#   vae_loss = K.mean(reconstruction_loss + kl_loss)
#   vae.add_loss(vae_loss)
#   vae.compile(optimizer='adam')

#   callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=5, patience=10)]
#   vae.fit(xtra_ac, xtra_ac, epochs=500, batch_size=128, shuffle=True,validation_data=(xval_ac, xval_ac), callbacks=callbacks)

#   # model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])
#   # callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)]



#   encoded_features_tr = encoder(xtra_ac)
#   #decoded_features_train=decoder((encoded_features_train)[2])
#   encoded_features_va = encoder(xval_ac)
#   #decoded_features_val=decoder((encoded_features_val)[2])
#   encoded_features_train=np.array(encoded_features_tr)
#   encoded_features_train= encoded_features_train[2,:]
#   encoded_features_train=np.array(encoded_features_train)
#   # print(encoded_features_train.shape)
#   encoded_features_val=np.array(encoded_features_va)
#   encoded_features_val= encoded_features_val[2,:]
#   encoded_features_val=np.array(encoded_features_val)


#   model= keras.Sequential()
#   model.add(Dense(128, activation='relu'))
#   model.add(Dropout(0.45))
#   model.add(Dense(64,  activation='relu'))
#   model.add(Dense(32,  activation='relu'))
#   model.add(Dense(16,  activation='relu'))
#   model.add(Dense(8,  activation='relu'))
#   model.add(Dense(4, activation='relu'))
#   model.add(Dense(2, activation='relu'))
#   model.add(Dense(1, activation='sigmoid'))

#   model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])
#   callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)]
#   model.fit(encoded_features_train, ytra_ac, epochs=500, batch_size=128, validation_data=(encoded_features_val, yval_ac), callbacks=callbacks)
#   #model.load_weights('best_model_weights.h5')

#   encoded_features_ts1 = encoder(xtest_ac)
#   encoded_features_test1=np.array(encoded_features_ts1)
#   encoded_features_test1= encoded_features_test1[2,:]
#   encoded_features_test1=np.array(encoded_features_test1)

#   pred_ger_labels = model(encoded_features_test1)
#   pred_ger_labels = pred_ger_labels.numpy()
#   post_ger_labels = make_partitions(wtest1, pred_ger_labels)
#   pred_ger_labels1 = convert_function(pred_ger_labels)

#   var1 = calculate_accuracy(pred_ger_labels1, ytest1)
#   var2 = calculate_accuracy(post_ger_labels, ytest1)
#   var3 = f1_score(pred_ger_labels1, ytest1)
#   var4 = f1_score(post_ger_labels, ytest1)


#   if var1 > highest_accuracy:
#         highest_accuracy = var1

#         # Save the model weights after achieving a new highest accuracy
#         model.save_weights('best_model_weights1.h5')


#   woPP.append(round(var1,3)*100)
#   wPP.append(round(var2,3)*100)
#   f1score_woPP.append(round(var3,3)*100)
#   f1score_wPP.append(round(var4,3)*100)

# print(woPP)
# '''average1 = statistics.mean(woPP)
# std1 = statistics.stdev(woPP)
# print("Average:", average1)
# print("Standard Deviation:", std1)'''

# print(wPP)
# '''average2 = statistics.mean(wPP)
# std2 = statistics.stdev(wPP)
# print("Average:", average2)
# print("Standard Deviation:", std2)'''

# print(f1score_woPP)
# '''average3 = statistics.mean(f1score_woPP)
# std3 = statistics.stdev(f1score_woPP)
# print("Average:", average3)
# print("Standard Deviation:", std3)'''

# print(f1score_wPP)
# '''average4 = statistics.mean(f1score_wPP)
# std4 = statistics.stdev(f1score_wPP)
# print("Average:", average4)
# print("Standard Deviation:", std4)'''

xtrain1, ytrain1 = dl("GER_AC_features_TDNN.mat")
xtest_ac, ytest, wtest, test_ind = dl("GER_test_AC_features_TDNN.mat")
xt, yt, wt = dl("GER_test_AC_features.mat")

print("TDNN train data features:", xtrain1.shape, "\nTDNN train data labels:", ytrain1.shape)
print("TDNN test data features:", xtest_ac.shape, "\nTDNN test data labels: ",ytest.shape, "\nTDNN test data syllable count: ",wtest.shape, "\nVAE_DNN test data features: ",xt.shape, "\nVAE_DNN test data labels: ",yt.shape, "\nVAE_DNN test data word count: ", wt.shape)

yt = yt.flatten()
wtest = wtest.flatten()
test_ind = test_ind.flatten()
print(yt.shape, wtest.shape, test_ind.shape)

original_dim = 38

woPP = []
wPP = []
accuracy_all_folds_wopp_a = []
accuracy_all_folds_wpp_a = []
f1score_all_folds_wopp_a = []
f1score_all_folds_wpp_a = []
accuracy_all_folds_wpp_a1 = []
j=0
#xtra_ac, xval_ac, ytra_ac, yval_ac = train_test_split(xtrain1, ytrain1, test_size=0.2, random_state=42)

train_size = xtrain1.shape[0]
print(train_size)

max_sequence_length = 5
sequence_length = 5

for j in range(0,1): # folds

  print(f"\nFold {j + 1} - Max Sequence Length: {max_sequence_length}")

  xval_ac = xtrain1[(train_size*j)//5:(train_size*(j+1))//5];
  print(xval_ac.shape)
  yval_ac = ytrain1[(train_size*j)//5:(train_size*(j+1))//5]
  xtra_ac = np.concatenate((xtrain1[:(train_size*j)//5], xtrain1[((train_size*(j+1))//5):]), axis=0);
  ytra_ac = np.concatenate((ytrain1[:(train_size*j)//5], ytrain1[((train_size*(j+1))//5):]), axis=0)


  input_shape = (max_sequence_length, original_dim)
  lab_input = Input(shape=(sequence_length, 1), name="lab_input")
  td_input = Input(shape=(sequence_length, original_dim), name="input")

  td_h = TimeDistributed(Dense(intermediate_dim, activation='relu'))(td_input)
  td_z_mean = TimeDistributed(Dense(latent_dim))(td_h)
  td_z_log_sigma = TimeDistributed(Dense(latent_dim))(td_h)

  td_z = Lambda(sampling)([td_z_mean, td_z_log_sigma])

  td_x = TimeDistributed(Dense(intermediate_dim, activation='relu'))(td_z)
  td_d_pred = TimeDistributed(Dense(original_dim, activation='sigmoid', name="same"))(td_x)


  callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=5, patience=10)]
  vae.fit(xtra_ac, xtra_ac, epochs=500, batch_size=128, shuffle=True,validation_data=(xval_ac, xval_ac), callbacks=callbacks)

  print(xtra_ac.shape, xval_ac.shape)


  model1 = Sequential()
  model1.add(Masking(mask_value=-1, input_shape=(max_sequence_length, original_dim)))
  model1.add(TimeDistributed(Dense(128, activation='relu')))
  model1.add(TimeDistributed(Dropout(0.45)))
  model1.add(TimeDistributed(Dense(64, activation='relu')))
  model1.add(TimeDistributed(Dense(32, activation='relu')))
  model1.add(TimeDistributed(Dense(16, activation='relu')))
  model1.add(TimeDistributed(Dense(8, activation='relu')))
  model1.add(TimeDistributed(Dense(4, activation='relu')))
  model1.add(TimeDistributed(Dense(2, activation='relu')))
  model1.add(TimeDistributed(Dense(1, activation='sigmoid')))

  model1.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])


  sequence_length = 5
  num_sequences = len(xtra_ac) // sequence_length

  xtra_ac_reshaped = xtra_ac[:num_sequences * sequence_length].reshape((num_sequences, sequence_length, 38))
  print(xtra_ac_reshaped.shape)

  ytra_ac_modified = np.zeros((num_sequences, sequence_length, 1), dtype=int)

  ytra_ac_reshaped = ytra_ac.reshape((num_sequences, sequence_length, 1))

  ytra_ac_modified[:, :, 0] = ytra_ac_reshaped[:, :, 0]

  num_sequences_val = len(xval_ac) // sequence_length

  xval_ac_reshaped = xval_ac[:num_sequences_val * sequence_length].reshape((num_sequences_val, sequence_length, 38))

  i=0; j = 0; label = 0

  yval_ac_modified = np.zeros((num_sequences_val, sequence_length, 1), dtype=int)

  yval_ac_reshaped = yval_ac.reshape((num_sequences_val, sequence_length, 1))

  yval_ac_modified[:, :, 0] = yval_ac_reshaped[:, :, 0]

  early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

  model1.fit(xtra_ac_reshaped, ytra_ac_modified, epochs=500, batch_size=32, validation_data=(xval_ac_reshaped, yval_ac_modified), callbacks=[early_stopping])



  num_sequences_test = len(xtest_ac) // sequence_length

  xtest_ac_modified = xtest_ac[:num_sequences_test * sequence_length].reshape((num_sequences_test, sequence_length, 38))
  i=0; j = 0; label = 0

  pred_output1= model1.predict(xtest_ac_modified)
  # print(pred_output1)
  array_of_arrays = pred_output1

  flattened_array = array_of_arrays.reshape(-1, array_of_arrays.shape[-1])

  result_array_ = [flattened_array[i] for i in range(len(flattened_array)) if i not in test_ind]
  #print(result_array)
  result_array = np.array([array.tolist() for array in result_array_])

  y_pred_a = np.where(result_array > 0.5, 1, 0)

  accuracy = calculate_accuracy(y_pred_a, yt)
  print("============......",accuracy)
  post_labels=make_partitions(wtest, result_array)
  post_l = np.array(post_labels)

  accuracy_xtest_a = calculate_accuracy(post_l, yt)
  print(accuracy_xtest_a,"original")
  # F1_score_WoPP_a = f1_score(ytest1, y_pred_a)
  # F1_score_WPP_a = f1_score(ytest1, post_labels)

  accuracy_all_folds_wopp_a.append(round(accuracy * 100, 3))
  accuracy_all_folds_wpp_a.append(round(accuracy_xtest_a * 100, 3))
  # f1score_all_folds_wopp_a.append(round(F1_score_WoPP_a * 100, 3))
  # f1score_all_folds_wpp_a.append(round(F1_score_WPP_a * 100, 3))