# -*- coding: utf-8 -*-
"""PostNet on J_VAE_DNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aEYNgSNppXHQdOD33ompOv4L9gubZJ7Z
"""

# from google.colab import drive
# drive.mount('/content/drive')

import os
import keras
import scipy.io
import statistics
import numpy as np
from keras import layers
import tensorflow as tf
import keras.backend as K
from tensorflow import keras
from keras.models import Model
from collections import Counter
from sklearn.metrics import f1_score
from keras.utils import to_categorical
from keras.optimizers import legacy, Adam
from keras.preprocessing.sequence import pad_sequences
from keras.callbacks import ModelCheckpoint ,EarlyStopping
from keras.layers import Input, TimeDistributed, Dense, Dropout, Lambda, Masking, BatchNormalization, concatenate

#loading data

def dl(path):
  data = scipy.io.loadmat(path)
  print(data.keys())
  x = data['features']; y = data['labels'];
  if 'test' in path and 'TDNN' in path:
    w = data['w']
    test_ind = data['test_ind']
    return x, y, w, test_ind
  elif 'test' in path and 'TDNN' not in path:
    w = data['w']
    return x, y, w
  else:
    return x, y.T

def normalization(feats,avg,std):
  ii=0
  for v in feats:
    # print(len(v))
    feats[ii] = np.divide((v-avg),std)
    ii = ii+1
  return feats

# post processing the labels
def make_partitions(arr_words, arr_labels):
  v=[]
  np.array(v)
  temp=[]
  for i in range(len(arr_words)-1):
    word=arr_words[i]
    next_word=arr_words[i+1]
    temp.append(arr_labels[i])
    if word!=next_word:
      numpy_temp=np.array(temp)
      temp_max=np.amax(numpy_temp)
      numpy_temp=np.divide(numpy_temp, temp_max)
      v=np.concatenate((v, numpy_temp), axis=None)
      temp.clear()
    if (i==len(arr_words)-2):
      temp.append(arr_labels[i+1])
      numpy_temp=np.array(temp)
      temp_max=np.amax(numpy_temp)
      numpy_temp=np.divide(numpy_temp, temp_max)
      v=np.concatenate((v, numpy_temp), axis=None)
      temp.clear()
  v1=[]
  for i in v:
    if i==1:
      v1.append(1)
    else:
      v1.append(0)
  return v1

#Function to compute classification accuracy
def calculate_accuracy(arr1, arr2):
  count=0
  for itr1, itr2 in zip(arr1, arr2):
    if itr1==itr2:
      count+=1
  return count/len(arr1)

def sampling(args):
    z_mean, z_log_sigma = args
    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=0.004)
    return z_mean + K.exp(z_log_sigma) * epsilon

filee = 'GER_AC_features_TDNN.mat'

print('Classification with::::::',os.path.basename(filee))

train_path = filee;
xtrain1, ytrain1 = dl(train_path);

train_size = xtrain1.shape[0]
print(train_size)

avg_trainfeat1=np.mean(xtrain1, axis=0)
std_trainfeat1=np.std(xtrain1, axis=0)

test_path = 'GER_test_AC_features.mat'
print('test file:::::::',os.path.basename(test_path))

xtest1, ytest1, wtest1 = dl(test_path);

xtest_ac = normalization(xtest1,avg_trainfeat1,std_trainfeat1)

xtrain1 = normalization(xtrain1,avg_trainfeat1,std_trainfeat1)
print(xtrain1.shape)

woPP=[]; wPP=[]
accuracy_all_folds_wopp = []; accuracy_all_folds_wpp = []; f1score_all_folds_wopp = []; f1score_all_folds_wpp = [];
accuracy_all_folds_wopp_a = [];  accuracy_all_folds_wpp_a  = [];  f1score_all_folds_wopp_a  = [];  f1score_all_folds_wpp_a  = [];


print(xtrain1.shape)
print(ytrain1.shape)

highest_accuracy = 0.0

# original_dim = 38
# intermediate_dim = 64
# latent_dim = 19

# woPP = []; wPP = []; f1score_woPP =[]; f1score_wPP=[]
# req_val2 = []
# ytest_ac_one_hot = tf.keras.utils.to_categorical(ytest1, num_classes=2)

# for j in range(0,1): # folds

#   xval_ac = xtrain1[(train_size*j)//5:(train_size*(j+1))//5];
#   print(xval_ac.shape)
#   yval_ac = ytrain1[(train_size*j)//5:(train_size*(j+1))//5]
#   xtra_ac = np.concatenate((xtrain1[:(train_size*j)//5], xtrain1[((train_size*(j+1))//5):]), axis=0);
#   ytra_ac = np.concatenate((ytrain1[:(train_size*j)//5], ytrain1[((train_size*(j+1))//5):]), axis=0)

#   lab_input = keras.Input(shape=(1,), name="lab_input")
#   ae_input = keras.Input(shape=(original_dim,), name="input")

#   h = layers.Dense(intermediate_dim, activation='relu')(ae_input)
#   z_mean = layers.Dense(latent_dim)(h)
#   z_log_sigma = layers.Dense(latent_dim)(h)

#   z = layers.Lambda(sampling)([z_mean, z_log_sigma])

# #print (z.shape)
#   encoder = keras.Model(inputs=[ae_input, lab_input],
#                         outputs=z
#                         )


#   x = Dense(intermediate_dim, activation='relu')(z)
#   d_pred = Dense(original_dim, activation='sigmoid', name="same")(x)
#   l=tf.concat([z, ae_input], 1)
#   clf_features=Dense(128, activation='relu')(l)
#   clf_features=Dropout(0.3)(clf_features)
#   clf_features=Dense(64, activation='relu')(clf_features)
#   clf_features=Dropout(0.2)(clf_features)
#   clf_features=Dense(32, activation='relu')(clf_features)
#   #clf_features=Dropout(0.45)(clf_features)
# #  clf_features=Dense(16, activation='relu')(clf_features)
# #  clf_features=Dense(8, activation='relu')(clf_features)
#   clf_features=Dense(4, activation='relu')(clf_features)
# #  clf_features=Dense(2, activation='relu')(clf_features)
#   clf_pred=Dense(2, activation='softmax', name='stress')(clf_features)

#   print(ae_input, d_pred)


#   reconstruction_loss = keras.losses.mean_squared_error(ae_input, d_pred)
#   reconstruction_loss *= 19
#   kl_loss = 1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma)
#   kl_loss = K.sum(kl_loss, axis=-1)
#   kl_loss *= -0.5
#   vae_loss = K.mean(reconstruction_loss + kl_loss)

#   l1=keras.losses.binary_crossentropy(lab_input ,clf_pred)
#   clf_loss = K.mean(l1)

#   model = keras.Model(
#       inputs=[ae_input, lab_input],
#       outputs=[d_pred, clf_pred],
#   )
#   model.add_loss({"same":vae_loss})
#   model.add_loss({"stress":clf_loss})
#   model.compile(
#       optimizer=adam,
#       loss_weights={"same": 1, "stress":1},
#   )

#   callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)]

#   model.summary()

#   ytra_ac_one_hot = tf.keras.utils.to_categorical(ytra_ac, num_classes=2)
#   yval_ac_one_hot = tf.keras.utils.to_categorical(yval_ac, num_classes=2)

#   model.fit({"input": xtra_ac, "lab_input": ytra_ac_one_hot}, {"same": xtra_ac, "stress": ytra_ac_one_hot},
#           epochs=200, batch_size=32,
#           validation_data=({"input": xval_ac, "lab_input": yval_ac_one_hot}, {"same": xval_ac, "stress": yval_ac_one_hot}),
#           callbacks=callbacks)

#   pred_output=model([xtest_ac, ytest_ac_one_hot])
#  # print(pred_output)
#   pred_labels=pred_output[1].numpy()
#   pred1_labels=pred_output[1].numpy()
#   for i in range (len(pred1_labels)):
#     if pred1_labels[i][1]>0.5:
#       pred1_labels[i][1]=1
#     else:
#       pred1_labels[i][1]=0
#   post_labels=make_partitions(wtest1, pred_labels)
#   test_swapped_array = [1.0 if x == 0 else 0.0 for x in post_labels]


#   p = pred1_labels[1].flatten()
#   var1 = calculate_accuracy(pred1_labels[:,1], ytest1)
#   var2 = calculate_accuracy(test_swapped_array, ytest1)
#   var3 = f1_score(pred1_labels[:,1], ytest1)
#   var4 = f1_score(test_swapped_array, ytest1)

#   accuracy_xtest_a=calculate_accuracy(test_swapped_array, ytest1)
#   if var2 > highest_accuracy:
#         highest_accuracy = var2

#         # Save the model weights after achieving a new highest accuracy
#         model.save_weights('best_model_weights.h5')

#   woPP.append(round(var1,3)*100)
#   wPP.append(round(var2,3)*100)
#   f1score_woPP.append(round(var3,3)*100)
#   f1score_wPP.append(round(var4,3)*100)

# print(woPP)
# '''average1 = statistics.mean(woPP)
# std1 = statistics.stdev(woPP)
# print("Average:", average1)
# print("Standard Deviation:", std1)'''

# print(100-wPP[0])
# '''average2 = statistics.mean(wPP)
# std2 = statistics.stdev(wPP)
# print("Average:", average2)
# print("Standard Deviation:", std2)'''

# print(f1score_woPP)
# '''average3 = statistics.mean(f1score_woPP)
# std3 = statistics.stdev(f1score_woPP)
# print("Average:", average3)
# print("Standard Deviation:", std3)'''

# print(100-f1score_wPP[0])
# '''average4 = statistics.mean(f1score_wPP)
# std4 = statistics.stdev(f1score_wPP)
# print("Average:", average4)
# print("Standard Deviation:", std4)'''

xtrain1, ytrain1 = dl("GER_AC_features_TDNN.mat")
xtest_ac, ytest, wtest, test_ind = dl("GER_test_AC_features_TDNN.mat")
xt, yt, wt = dl("GER_test_AC_features.mat")

print("TDNN train data features:", xtrain1.shape, "\nTDNN train data labels:", ytrain1.shape)
print("TDNN test data features:", xtest_ac.shape, "\nTDNN test data labels: ",ytest.shape, "\nTDNN test data syllable count: ",wtest.shape, "\nJ_VAE_DNN test data features: ",xt.shape, "\nJ_VAE_DNN test data labels: ",yt.shape, "\nJ_VAE_DNN test data word count: ", wt.shape)

yt = yt.flatten()
wtest = wtest.flatten()
test_ind = test_ind.flatten()
print(yt.shape, wtest.shape, test_ind.shape)

original_dim = 38
sequence_length = 5
intermediate_dim = 64
latent_dim = 19


woPP = []
wPP = []
accuracy_all_folds_wopp_a = []
accuracy_all_folds_wpp_a = []
f1score_all_folds_wopp_a = []
f1score_all_folds_wpp_a = []
accuracy_all_folds_wpp_a1 = []
j=0

train_size = xtrain1.shape[0]
print(train_size)

for j in range(0,1): # folds
  xval_ac = xtrain1[(train_size*j)//5:(train_size*(j+1))//5];
  print(xval_ac.shape)
  yval_ac = ytrain1[(train_size*j)//5:(train_size*(j+1))//5]
  xtra_ac = np.concatenate((xtrain1[:(train_size*j)//5], xtrain1[((train_size*(j+1))//5):]), axis=0);
  ytra_ac = np.concatenate((ytrain1[:(train_size*j)//5], ytrain1[((train_size*(j+1))//5):]), axis=0)


  max_sequence_length = 5
  print(f"\nMax Sequence Length: {max_sequence_length}")

  def sampling(args):
      z_mean, z_log_sigma = args
      batch = K.shape(z_mean)[0]
      time_steps = K.shape(z_mean)[1]
      dim = K.int_shape(z_mean)[2]
      epsilon = K.random_normal(shape=(batch, time_steps, dim), mean=0., stddev=0.004)
      return z_mean + K.exp(z_log_sigma) * epsilon

  input_shape = (max_sequence_length, original_dim)
  lab_input = Input(shape=(sequence_length, 1), name="lab_input")
  td_input = Input(shape=(sequence_length, original_dim), name="input")

  td_h = TimeDistributed(Dense(intermediate_dim, activation='relu'))(td_input)
  td_z_mean = TimeDistributed(Dense(latent_dim))(td_h)
  td_z_log_sigma = TimeDistributed(Dense(latent_dim))(td_h)

  td_z = Lambda(sampling)([td_z_mean, td_z_log_sigma])

  td_x = TimeDistributed(Dense(intermediate_dim, activation='relu'))(td_z)
  td_d_pred = TimeDistributed(Dense(original_dim, activation='sigmoid', name="same"))(td_x)

  td_l = tf.concat([td_z, td_input], 2)
  td_clf_features = Masking(mask_value=-1, input_shape=(max_sequence_length, original_dim))(td_l)
  td_clf_features = TimeDistributed(Dense(128, activation='relu'))(td_clf_features)
  td_clf_features = TimeDistributed(Dropout(0.3))(td_clf_features)
  td_clf_features = TimeDistributed(Dense(64, activation='relu'))(td_clf_features)
  td_clf_features = TimeDistributed(Dropout(0.2))(td_clf_features)
  td_clf_features = TimeDistributed(Dense(32, activation='relu'))(td_clf_features)
  td_clf_features = TimeDistributed(Dense(4, activation='relu'))(td_clf_features)
  td_clf_pred = TimeDistributed(Dense(2, activation='softmax', name='stress'))(td_clf_features)

  reconstruction_loss = K.mean(K.square(td_input - td_d_pred)) * 19
  kl_loss = 1 + td_z_log_sigma - K.square(td_z_mean) - K.exp(td_z_log_sigma)
  kl_loss = K.sum(kl_loss, axis=-1) * -0.5
  vae_loss = K.mean(reconstruction_loss + kl_loss)

  l1 = K.mean(K.binary_crossentropy(lab_input, td_clf_pred))
  clf_loss = l1

  combined_loss = vae_loss + clf_loss

  combined_model = Model(inputs=[td_input, lab_input], outputs={'same': td_d_pred, 'stress': td_clf_pred})
  combined_model.add_loss({"same": vae_loss})
  combined_model.add_loss({"stress": clf_loss})

  combined_model.load_weights('Joint_VAE_weights.h5')

  combined_model.compile(
      optimizer=Adam(learning_rate=0.005),
      loss={'same': None, 'stress': None},
      metrics={'stress': 'accuracy'}
  )

  num_sequences = len(xtra_ac) // sequence_length
  xtra_ac_reshaped = xtra_ac[:num_sequences * sequence_length].reshape((num_sequences, sequence_length, 38))

  ytra_ac_modified = np.zeros((num_sequences, sequence_length, 1), dtype=int)
  ytra_ac_reshaped = ytra_ac.reshape((num_sequences, sequence_length, 1))
  ytra_ac_modified[:, :, 0] = ytra_ac_reshaped[:, :, 0]

  num_sequences_val = len(xval_ac) // sequence_length
  xval_ac_reshaped = xval_ac[:num_sequences_val * sequence_length].reshape((num_sequences_val, sequence_length, 38))

  yval_ac_modified = np.zeros((num_sequences_val, sequence_length, 1), dtype=int)
  yval_ac_reshaped = yval_ac.reshape((num_sequences_val, sequence_length, 1))
  yval_ac_modified[:, :, 0] = yval_ac_reshaped[:, :, 0]

  ytrain_ac_one_hot = tf.keras.utils.to_categorical(ytra_ac_modified, num_classes=2)
  yval_ac_one_hot = tf.keras.utils.to_categorical(yval_ac_modified, num_classes=2)

  early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

  combined_model.fit(
      {"input": xtra_ac_reshaped, "lab_input": ytrain_ac_one_hot},
      {"same": xtra_ac_reshaped, "stress": ytrain_ac_one_hot},
      epochs=500, batch_size=32,
      validation_data=({"input": xval_ac_reshaped, "lab_input": yval_ac_one_hot},
                      {"same": xval_ac_reshaped, "stress": yval_ac_one_hot}),
      callbacks=[early_stopping]
  )

num_sequences_test = len(xtest_ac) // sequence_length

xtest_ac_modified = xtest_ac[:num_sequences_test * sequence_length].reshape((num_sequences_test, sequence_length, 38))
i=0; j = 0; label = 0

ytest_ac_modified = np.zeros((num_sequences_test, sequence_length, 1), dtype=int)


ytest_ac_reshaped = ytest.reshape((num_sequences_test, sequence_length, 1))

ytest_ac_modified[:, :, 0] = ytest_ac_reshaped[:, :, 0]

ytest_ac_one_hot = tf.keras.utils.to_categorical(ytest_ac_modified, num_classes=2)

pred_output=combined_model([xtest_ac_modified, ytest_ac_one_hot])
pred1_labels=pred_output['stress'].numpy()

array_of_arrays = pred1_labels

flattened_array = array_of_arrays.reshape(-1, array_of_arrays.shape[-1])

result_array_ = [flattened_array[i] for i in range(len(flattened_array)) if i not in test_ind]
result_array = np.array([array.tolist() for array in result_array_])
# print(result_array)

y_pred_a = np.where(result_array[:,1] > 0.5, 1, 0)

accuracy = calculate_accuracy(y_pred_a, yt)
print("============......",accuracy)
post_labels=make_partitions(wtest, result_array[:,1])
post_l = np.array(post_labels)
swapped_array = np.where(post_l == 0, 1.0, 0.0)

accuracy_xtest_a = calculate_accuracy(post_l, yt)
accuracy_xtest_a1 = calculate_accuracy(swapped_array, yt)
print(accuracy_xtest_a,"original")
# F1_score_WoPP_a = f1_score(ytest1, y_pred_a)
# F1_score_WPP_a = f1_score(ytest1, post_labels)

accuracy_all_folds_wopp_a.append(round(accuracy * 100, 3))
accuracy_all_folds_wpp_a.append(round(accuracy_xtest_a * 100, 3))
accuracy_all_folds_wpp_a1.append(round(accuracy_xtest_a1 * 100, 3))
# f1score_all_folds_wopp_a.append(round(F1_score_WoPP_a * 100, 3))
# f1score_all_folds_wpp_a.append(round(F1_score_WPP_a * 100, 3))